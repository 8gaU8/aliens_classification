{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3587678e-3eb4-4614-93e2-f072d6256d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.compiler.mlcompute import mlcompute\n",
    "mlcompute.set_mlc_device(device_name=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608db5d5-7d17-47e2-8ab5-24b4aa00abb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973a5e59-4abb-433a-86b0-ea83f3bcf0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8861254-cac1-424b-aa97-476d4626a945",
   "metadata": {},
   "source": [
    "## 参考\n",
    "https://www.tensorflow.org/guide/keras/transfer_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af7a9b9-a03a-44c8-9a4b-0e09f5577df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463dee30-19a6-4965-80a8-5eaeb38d74eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pathlib.Path('./images').iterdir():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbaa188-0348-4076-845b-cacf9bd580f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = pathlib.Path( os.getcwd() +  '/images')\n",
    "\n",
    "train_path = data_root / 'aug_train'\n",
    "valid_path = data_root / 'aug_valid'\n",
    "\n",
    "train_image_paths = list(train_path.glob('*/*'))\n",
    "train_image_paths = [str(path) for path in train_image_paths]\n",
    "random.shuffle(train_image_paths)\n",
    "print(len(train_image_paths))\n",
    "\n",
    "valid_image_paths = list(valid_path.glob('*/*'))\n",
    "valid_image_paths = [str(path) for path in valid_image_paths]\n",
    "random.shuffle(valid_image_paths)\n",
    "print(len(valid_image_paths))\n",
    "\n",
    "path = train_image_paths[0]\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355aa672-aee5-4db3-a4ee-fb6afc00afa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = sorted(item.name for item in train_path.glob('*/') if item.is_dir())\n",
    "label_to_index = dict((name, index) for index, name in enumerate(label_names))\n",
    "\n",
    "train_image_labels = [label_to_index[pathlib.Path(path).parent.name]\n",
    "                      for path in train_image_paths]\n",
    "valid_image_labels = [label_to_index[pathlib.Path(path).parent.name]\n",
    "                      for path in valid_image_paths]\n",
    "\n",
    "print(train_image_labels[:10])\n",
    "print(valid_image_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3318172d-b118-4b12-99dd-694ec57677b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ed925-b6c5-4a48-8593-d25b7dceafd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "#     image = tf.image.resize_with_pad(image, 192, 192)\n",
    "    image = tf.image.resize(image, [192, 192])\n",
    "    image /= 255.0\n",
    "#     image = 2*image-1\n",
    "    return image\n",
    "\n",
    "def load_and_preprocess_image(path):\n",
    "    image = tf.io.read_file(path)\n",
    "\n",
    "    return preprocess_image(image)\n",
    "\n",
    "def load_and_preprocess_from_path_label(path, label):\n",
    "    return load_and_preprocess_image(path), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6537fd-7246-4676-a968-de637b63dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = tf.data.Dataset.from_tensor_slices( (train_image_paths, train_image_labels))\n",
    "# valid_ds = tf.data.Dataset.from_tensor_slices( (valid_image_paths, valid_image_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92948b56-dcca-405a-b0ea-452e3b19873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_image_label_ds = train_ds.map(load_and_preprocess_from_path_label)\n",
    "# valid_image_label_ds = valid_ds.map(load_and_preprocess_from_path_label)\n",
    "# train_image_label_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf268f6-3235-43b8-929d-a2cf48938789",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_ds = tf.data.Dataset.from_tensor_slices(train_image_paths)\n",
    "valid_path_ds = tf.data.Dataset.from_tensor_slices(valid_image_paths)\n",
    "\n",
    "train_image_ds = train_path_ds.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "valid_image_ds = valid_path_ds.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4290ae9-6d38-460b-89c8-f2362e3183e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "for n,image in enumerate(train_image_ds.take(4)):\n",
    "    plt.subplot(2,2,n+1)\n",
    "    plt.imshow(image)\n",
    "    plt.grid(False)\n",
    "    print(image.shape)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "#     plt.xlabel(caption_image(train_image_paths[n]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec1aaf-edcf-4ee5-9161-9af03125c707",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(train_image_labels, tf.int64))\n",
    "valid_label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(valid_image_labels, tf.int64))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1030be-5f7d-414c-b482-bf78f5b3ac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_label_ds = tf.data.Dataset.zip((train_image_ds, train_label_ds))\n",
    "valid_image_label_ds = tf.data.Dataset.zip((valid_image_ds, valid_label_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41bf816-3cf4-4efe-a246-e5eee895f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_ds = train_image_label_ds.shuffle(buffer_size=len(train_image_paths)).batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
    "valid_ds = valid_image_label_ds.shuffle(buffer_size=len(valid_image_paths)).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc36e701-256a-44fb-91c5-4b92d26eb3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_net = tf.keras.applications.MobileNetV2(input_shape=(192, 192, 3), include_top=False, weights='imagenet')\n",
    "mobile_net.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facd158d-befa-442c-9b58-6b583572809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_range(image, label):\n",
    "    return 2*image-1, label\n",
    "\n",
    "train_ds = train_ds.map(change_range)\n",
    "valid_ds = valid_ds.map(change_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7426b8e-889b-4cae-8360-d60683ada2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.Sequential([\n",
    "#     mobile_net(),\n",
    "#     tf.keras.layers.GlobalAveragePooling2D(),\n",
    "#     tf.keras.layers.Dropout(0.2),\n",
    "#     tf.keras.layers.Dense(2),\n",
    "# ])\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "prediction_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(192,192,3))\n",
    "x = mobile_net(inputs, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.25)(x)\n",
    "outputs = prediction_layer(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a85d72e-36ae-490f-befc-8cb28d08c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_learning_rate = 0.001\n",
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba068ac1-122e-4bc9-8328-2e0b4b668d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch=tf.math.ceil(len(train_image_paths)/BATCH_SIZE).numpy()\n",
    "model.fit(train_ds, epochs=1, steps_per_epoch=steps_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed18355-3371-4da9-8225-2126b515ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, epochs=20, validation_data=valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e3e6a9-28cd-4eff-9155-5c7e6ca5c203",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f14005-79f9-4425-ac7a-82eeb9aef960",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('transfer.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ac0a3b-88b5-46ff-b21a-638a4dde698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_net.trainable = True\n",
    "for layer in mobile_net.layers[:100]:\n",
    "    layer.trainable = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705d7a35-4539-4503-a828-0a890868b8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate/10),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e5e4c5-25bb-4b6e-ad75-20c2d71cf9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc34bb96-e9e4-40a8-97ba-a47b6b445b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_epochs = 10\n",
    "total_epochs = 10 + fine_tune_epochs\n",
    "\n",
    "history_fine = model.fit(train_ds, \n",
    "                        epochs=total_epochs,\n",
    "                        initial_epoch=history.epoch[-1],\n",
    "                        validation_data=valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600140ea-2e0a-4a17-b825-7fcca1a2e26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc += history_fine.history['accuracy']\n",
    "val_acc += history_fine.history['val_accuracy']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']\n",
    "initial_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f102348-f316-428b-a02e-1fc991e2debf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.ylim([0.8, 1])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 1.0])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dc9696-9e43-470b-b91d-0d24198caf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"~/ML/tfjs/dalek_or_cyberman/modeltf/tuned_model.h5\"\n",
    "model_path = \"transfer.h5\"\n",
    "\n",
    "model = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8240c9ac-a938-4d8c-841b-499069931fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "impath = '~/Downloads/dalek.jpg'\n",
    "impath = '~/Downloads/images-3.jpeg'\n",
    "# impath = './images/valid/dalek/d_000.jpg'\n",
    "impath = './images/train/cyberman/cyberman_000.jpg'\n",
    "\n",
    "size = (192, 192)\n",
    "# size = (160, 160)\n",
    "\n",
    "im = cv2.imread(impath)\n",
    "im = resize_with_pad(im, size)\n",
    "# im = cv2.resize(im, size)\n",
    "plt.imshow(im)\n",
    "\n",
    "im = im.reshape(1, *size, 3)\n",
    "# np.expand_dims(im, axis=)\n",
    "im.shape\n",
    "im = im/255\n",
    "im = im*2-1\n",
    "predictions = model.predict(im)\n",
    "\n",
    "predictions = tf.nn.sigmoid(predictions)\n",
    "\n",
    "# predictions = tf.where(predictions < 0.5, 0, 1)\n",
    "\n",
    "# predictions.numpy()[0][0]\n",
    "print(predictions.numpy()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84f078b-712f-483d-8baa-27fe82fdc201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b4727-7a76-49f9-9cdb-5be29d271317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(im):\n",
    "    h, w, _ = im.shape\n",
    "    if h == w:\n",
    "        return im\n",
    "    elif h > w:\n",
    "        pad = int((h-w)/2)\n",
    "        im = cv2.copyMakeBorder(im, 0, 0, pad, pad, cv2.BORDER_CONSTANT, (0, 0, 0))\n",
    "        return im\n",
    "    elif w > h:\n",
    "        pad = int((w-h)/2)\n",
    "        im = cv2.copyMakeBorder(im,  pad, pad, 0, 0, cv2.BORDER_CONSTANT, (0, 0, 0))\n",
    "        \n",
    "        return im\n",
    "def resize_with_pad(im, size):\n",
    "    im = padding(im)\n",
    "    im = cv2.resize(im, size)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dfea51-e87d-443e-a2c8-657c59381f75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
